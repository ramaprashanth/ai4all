{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Images\n",
    "\n",
    "With digital images or videos computers can be made to gain high-level understanding from digital images or videos and this is where Computer Vision comes into play.\n",
    "\n",
    "Computer Vision, often abbreviated as CV, is defined as a field of study that seeks to develop techniques to help computers “see” and understand the content of digital images such as photographs and videos.\n",
    "\n",
    "The problem of computer vision appears simple because it is trivially solved by people, even very young children. Nevertheless, it largely remains an unsolved problem based both on the limited understanding of biological vision and because of the complexity of vision perception in a dynamic and nearly infinitely varying physical world.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computer Vision\n",
    "\n",
    "We are awash in images.\n",
    "\n",
    "Smartphones have cameras, and taking a photo or video and sharing it has never been easier, resulting in the incredible growth of modern social networks like Instagram.\n",
    "\n",
    "YouTube might be the second largest search engine and hundreds of hours of video are uploaded every minute and billions of videos are watched every day.\n",
    "\n",
    "The internet is comprised of text and images. It is relatively straightforward to index and search text, but in order to index and search images, algorithms need to know what the images contain. For the longest time, the content of images and video has remained opaque, best described using the meta descriptions provided by the person that uploaded them.\n",
    "\n",
    "To get the most out of image data, we need computers to “see” an image and understand the content.\n",
    "\n",
    "This is a trivial problem for any human being.\n",
    "\n",
    "    * A person can describe the content of a photograph they have seen once.\n",
    "    * A person can summarize a video that they have only seen once.\n",
    "    * A person can recognize a face that they have only seen once before.\n",
    "\n",
    "\n",
    "Computer vision is a field of study focused on the problem of helping computers to see. At an abstract level, the goal of computer vision problems is to use the observed image data to infer something about the world."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/Computer-Vision.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenCV\n",
    "\n",
    "`OpenCV` is a library of programming functions mainly aimed at real-time computer vision. In addition to OpenCV, we will find ourselves use the following libraries in abundance as well. `Matplotlib` is an optional choice for displaying frames from video or images. We will show a couple of examples using it here. `Numpy` is used for all things \"numbers and Python.\"\n",
    "\n",
    "\n",
    "Lets go through few basic CV concepts.\n",
    "\n",
    "NOTE : `cv2.waitkey(0)` is given so that everytime an image window popsup, just press any key to close the window and the program."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RGB Color \n",
    "\n",
    "The red, green and blue light are added together in various ways to reproduce a broad array of colors and is widely used for sensing, representation, and display of images in electronic systems, such as televisions and computers.\n",
    "\n",
    "So a single RGB image pixel will have 3 values of R,G and B each ranging from 0-255 which points to their respective intensities. Say, if this image is converted to a gray scale image, its single pixel will have only one value of white intensity ranging from 0-255.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/rgb.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading an image\n",
    "\n",
    "To read the original image, simply call the `imread` function of the cv2 module, passing as input the path to the image, as a string.\n",
    "\n",
    "Run the following code which will read the image and open it in OpenCV as a popup window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "img = cv2.imread('./images/lenna.png')\n",
    "cv2.imshow('image',img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting an image to GrayScale\n",
    "\n",
    "Converting images to GrayScale will convert images from values of type RGB (Red,Blue & Green) to W/B (White/Black) which makes processing tasks like finding edges, contours, etc. much easier.\n",
    "\n",
    "For this, we need to call the `cvtColor` function, which allows to convert the image from a color space to another.\n",
    "\n",
    "As first input, this function receives the original image. As second input, it receives the color space conversion code. Since we want to convert our original image from the BGR color space to gray, we use the code `COLOR_BGR2GRAY`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "  \n",
    "img = cv2.imread('./images/lenna.png')\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "  \n",
    "cv2.imshow('Gray image', gray)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Edge Detection\n",
    "\n",
    "Edge detection is an image processing technique for finding the boundaries of objects within images. It works by detecting discontinuities in brightness. Edge detection is used for image segmentation and data extraction in computer vision.\n",
    "\n",
    "Canny Edge Detection is one such method used to detect the edges in an image. It accepts a gray scale image as input and it uses a multistage algorithm.\n",
    "\n",
    "You can perform this operation on an image using the `Canny()` method, following is the syntax of this method.\n",
    "\n",
    "`Canny(image, edges, threshold1, threshold2)`\n",
    "\n",
    "Parameters −\n",
    "\n",
    "    image − A Mat object representing the source (input image) for this operation.\n",
    "\n",
    "    edges − A Mat object representing the destination (edges) for this operation.\n",
    "\n",
    "    threshold1 − A variable of the type double representing the first threshold for the hysteresis procedure.\n",
    "\n",
    "    threshold2 − A variable of the type double representing the second threshold for the hysteresis procedure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "img = cv2.imread('./images/lenna.png')\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "edged = cv2.Canny(gray,30,200)\n",
    "cv2.imshow('Canny Edges',edged)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Contour Detection\n",
    "\n",
    "Contours can be explained simply as a curve joining all the continuous points (along the boundary), having same color or intensity. The contours are a useful tool for shape analysis and object detection and recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "img = cv2.imread('./images/lenna.png')\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "edged = cv2.Canny(gray,30,200)\n",
    "\n",
    "# findContours updates the edged variable\n",
    "img2, contours, hierarchy=cv2.findContours(edged,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_NONE)\n",
    "cv2.imshow('Canny Edges after Contouring', edged)\n",
    "\n",
    "# drawing all found contours\n",
    "cv2.drawContours(img, contours, -1, (0,255,0), 3)\n",
    "cv2.imshow('Contours', img)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numpy\n",
    "\n",
    "The NumPy package is the workhorse of data analysis, machine learning, and scientific computing in the python ecosystem. It vastly simplifies manipulating and crunching vectors and matrices. OpenCV relies on Numpy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Arrays\n",
    "<img src=\"./images/create-numpy-array-1.png\">\n",
    "<img src=\"./images/create-numpy-array-ones-zeros-random.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Array Arithmetic\n",
    "\n",
    "<img src=\"./images/numpy-array-subtract-multiply-divide.png\">\n",
    "<img src=\"./images/numpy-array-broadcast.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Array Indexing\n",
    "\n",
    "<img src=\"./images/numpy-array-slice.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Matrices\n",
    "\n",
    "<img src=\"./images/numpy-array-create-2d.png\">\n",
    "<img src=\"./images/numpy-matrix-ones-zeros-random.png\">\n",
    "<img src=\"./images/numpy-3d-array-creation.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Representation\n",
    "\n",
    "An image is a matrix of pixels of size (height x width).\n",
    "\n",
    "\n",
    "#### Grayscale Image\n",
    "\n",
    "If the image is black and white (a.k.a. grayscale), each pixel can be represented by a single number (commonly between 0 (black) and 255 (white)\n",
    "\n",
    "<img src=\"./images/numpy-grayscale-image.png\">\n",
    "\n",
    "\n",
    "#### Color Image\n",
    "\n",
    "If the image is colored, then each pixel is represented by three numbers - a value for each of Red, Green, and Blue. In that case we need a 3rd dimension (because each cell can only contain one number). So a colored image is represented by an ndarray of dimensions: (height x width x 3).\n",
    "\n",
    "<img src=\"./images/numpy-color-image.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember the color Leena image from before ? \n",
    "\n",
    "Let's try printing it and out to see how this color image is stored as a 3D (BGR) numpy matrix by OpenCV by running the snippet below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 512, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "img = cv2.imread('./images/lenna.png')\n",
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Image from NAO\n",
    "\n",
    "On the same note, once we get the image stream from NAO we will be able to process it and make sense of it.\n",
    "\n",
    "We get images from NAO's top camera and convert them to numpy array to process using OpenCV. This is done by Subscribing to `ALVideoDevice` proxy which gives raw image in form of pixel array which can be converted to numpy array using `np.asarray()` function and reshaped to a 3 dimensional color image of (height x width x 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subscribed to  nao_opencv_6\n",
      "Grabbed image:  320 x 240  numChannels= 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rama/.local/lib/python2.7/site-packages/ipykernel_launcher.py:45: DeprecationWarning: Assigning the 'data' attribute is an inherently unsafe operation and will be removed in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsubscribing  nao_opencv_6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from vision_definitions import kQVGA,kBGRColorSpace\n",
    "from naoqi import ALProxy\n",
    "\n",
    "NAO_IP=\"192.168.1.7\" \n",
    "# NAO_IP=\"<YOUR_NAO_IP>\"\n",
    "\n",
    "\n",
    "if __name__==\"__main__\":  # Should not run when imported\n",
    "\n",
    "    camera_index = 0 # Top camera\n",
    "   \n",
    "    # Proxy for ALVideoDevice\n",
    "    name = \"nao_opencv\"\n",
    "    video = ALProxy(\"ALVideoDevice\", NAO_IP, 9559)\n",
    "\n",
    "    # Subscribe to video device on a specific camera\n",
    "    # BGR for OpenCV\n",
    "    name = video.subscribeCamera(name,camera_index,kQVGA,kBGRColorSpace,30)\n",
    "    print \"Subscribed to \", name\n",
    "\n",
    "    try:\n",
    "        frame = None\n",
    "        # Keep Looping\n",
    "        while True:\n",
    "            # Get image\n",
    "            img = video.getImageRemote(name)\n",
    "\n",
    "            # Get image size attributes and pixel array buffer\n",
    "            imageWidth = img[0]\n",
    "            imageHeight = img[1]\n",
    "            numChannels = img[2]\n",
    "            imgBuffer = img[6]\n",
    "         \n",
    "            # Get OpenCV image (allocate on first pass)\n",
    "            if frame is None:\n",
    "                print 'Grabbed image: ',imageWidth,'x',imageHeight,' numChannels=',numChannels\n",
    "                frame=np.asarray(bytearray(imgBuffer), dtype=np.uint8)\n",
    "                frame=frame.reshape((imageHeight,imageWidth,3))\n",
    "            else:\n",
    "                frame.data=bytearray(imgBuffer)\n",
    "\n",
    "            # Display the frame to our screen\n",
    "            # NOTE : Do not run this code if your run your python in the robot\n",
    "            # as NAO has no screen to show\n",
    "            cv2.imshow(\"Frame\", frame)\n",
    "            \n",
    "            # Get the key pressed in the image window\n",
    "            key = cv2.waitKey(33)&0xFF\n",
    "            if  key == ord('q') or key == 27:\n",
    "                # Exit loop when 'q' or 'Esc' is pressed on the image window\n",
    "                break\n",
    "            \n",
    "    finally: # As fallback we'll make sure to unsubscribe\n",
    "        print \"Unsubscribing \",name\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "video.unsubscribe(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Now that we have got the image successfully, we proceed to recognize the gesture in the image."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
