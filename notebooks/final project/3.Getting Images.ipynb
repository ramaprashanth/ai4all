{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Images\n",
    "\n",
    "With digital images or videos computers can be made to gain high-level understanding from digital images or videos and this is where Computer Vision comes into play.\n",
    "\n",
    "Computer Vision, often abbreviated as CV, is defined as a field of study that seeks to develop techniques to help computers “see” and understand the content of digital images such as photographs and videos.\n",
    "\n",
    "The problem of computer vision appears simple because it is trivially solved by people, even very young children. Nevertheless, it largely remains an unsolved problem based both on the limited understanding of biological vision and because of the complexity of vision perception in a dynamic and nearly infinitely varying physical world.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computer Vision\n",
    "\n",
    "We are awash in images.\n",
    "\n",
    "Smartphones have cameras, and taking a photo or video and sharing it has never been easier, resulting in the incredible growth of modern social networks like Instagram.\n",
    "\n",
    "YouTube might be the second largest search engine and hundreds of hours of video are uploaded every minute and billions of videos are watched every day.\n",
    "\n",
    "The internet is comprised of text and images. It is relatively straightforward to index and search text, but in order to index and search images, algorithms need to know what the images contain. For the longest time, the content of images and video has remained opaque, best described using the meta descriptions provided by the person that uploaded them.\n",
    "\n",
    "To get the most out of image data, we need computers to “see” an image and understand the content.\n",
    "\n",
    "This is a trivial problem for any human being.\n",
    "\n",
    "    * A person can describe the content of a photograph they have seen once.\n",
    "    * A person can summarize a video that they have only seen once.\n",
    "    * A person can recognize a face that they have only seen once before.\n",
    "\n",
    "\n",
    "Computer vision is a field of study focused on the problem of helping computers to see. At an abstract level, the goal of computer vision problems is to use the observed image data to infer something about the world."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/Computer-Vision.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenCV\n",
    "\n",
    "`OpenCV` is a library of programming functions mainly aimed at real-time computer vision. In addition to OpenCV, we will find ourselves use the following libraries in abundance as well. `Matplotlib` is an optional choice for displaying frames from video or images. We will show a couple of examples using it here. `Numpy` is used for all things \"numbers and Python.\"\n",
    "\n",
    "\n",
    "Lets go through few basic CV concepts.\n",
    "\n",
    "NOTE : `cv2.waitkey(0)` is given so that everytime an image window popsup, just press any key to close the window and the program.\n",
    "\n",
    "So every time a window pops up, press `ESC` key. Do not close the window manually, because, if you do so, you will have to stop and restart the kernel. (Two buttons are right next to Run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RGB Color \n",
    "\n",
    "The red, green and blue light are added together in various ways to reproduce a broad array of colors and is widely used for sensing, representation, and display of images in electronic systems, such as televisions and computers.\n",
    "\n",
    "So a single RGB image pixel will have 3 values of R,G and B each ranging from 0-255 which points to their respective intensities. Say, if this image is converted to a gray scale image, its single pixel will have only one value of white intensity ranging from 0-255.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/rgb.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading an image\n",
    "\n",
    "To read the original image, simply call the `imread` function of the cv2 module, passing as input the path to the image, as a string.\n",
    "\n",
    "Run the following code which will read the image and open it in OpenCV as a popup window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "img = cv2.imread('./images/lenna.png')\n",
    "cv2.imshow('image',img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting an image to GrayScale\n",
    "\n",
    "Converting images to GrayScale will convert images from values of type RGB (Red,Blue & Green) to W/B (White/Black) which makes processing tasks like finding edges, contours, etc. much easier.\n",
    "\n",
    "For this, we need to call the `cvtColor` function, which allows to convert the image from a color space to another.\n",
    "\n",
    "As first input, this function receives the original image. As second input, it receives the color space conversion code. Since we want to convert our original image from the BGR color space to gray, we use the code `COLOR_BGR2GRAY`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "  \n",
    "img = cv2.imread('./images/lenna.png')\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "  \n",
    "cv2.imshow('Gray image', gray)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Edge Detection\n",
    "\n",
    "Edge detection is an image processing technique for finding the boundaries of objects within images. It works by detecting discontinuities in brightness. Edge detection is used for image segmentation and data extraction in computer vision.\n",
    "\n",
    "Canny Edge Detection is one such method used to detect the edges in an image. It accepts a gray scale image as input and it uses a multistage algorithm.\n",
    "\n",
    "You can perform this operation on an image using the `Canny()` method, following is the syntax of this method.\n",
    "\n",
    "`Canny(image, edges, threshold1, threshold2)`\n",
    "\n",
    "Parameters −\n",
    "\n",
    "    image − A Mat object representing the source (input image) for this operation.\n",
    "\n",
    "    edges − A Mat object representing the destination (edges) for this operation.\n",
    "\n",
    "    threshold1 − A variable of the type double representing the first threshold for the hysteresis procedure.\n",
    "\n",
    "    threshold2 − A variable of the type double representing the second threshold for the hysteresis procedure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "img = cv2.imread('./images/lenna.png')\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "edged = cv2.Canny(gray,30,200)\n",
    "cv2.imshow('Canny Edges',edged)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Contour Detection\n",
    "\n",
    "Contours can be explained simply as a curve joining all the continuous points (along the boundary), having same color or intensity. The contours are a useful tool for shape analysis and object detection and recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "img = cv2.imread('./images/lenna.png')\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "edged = cv2.Canny(gray,30,200)\n",
    "\n",
    "# findContours updates the edged variable\n",
    "img2, contours, hierarchy=cv2.findContours(edged,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_NONE)\n",
    "cv2.imshow('Canny Edges after Contouring', edged)\n",
    "\n",
    "# drawing all found contours\n",
    "cv2.drawContours(img, contours, -1, (0,255,0), 3)\n",
    "cv2.imshow('Contours', img)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numpy\n",
    "\n",
    "The NumPy package is the workhorse of data analysis, machine learning, and scientific computing in the python ecosystem. It vastly simplifies manipulating and crunching vectors and matrices. OpenCV relies on Numpy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Arrays\n",
    "<img src=\"./images/create-numpy-array-1.png\">\n",
    "<img src=\"./images/create-numpy-array-ones-zeros-random.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Array Arithmetic\n",
    "\n",
    "<img src=\"./images/numpy-array-subtract-multiply-divide.png\">\n",
    "<img src=\"./images/numpy-array-broadcast.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Array Indexing\n",
    "\n",
    "<img src=\"./images/numpy-array-slice.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Matrices\n",
    "\n",
    "<img src=\"./images/numpy-array-create-2d.png\">\n",
    "<img src=\"./images/numpy-matrix-ones-zeros-random.png\">\n",
    "<img src=\"./images/numpy-3d-array-creation.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Representation\n",
    "\n",
    "An image is a matrix of pixels of size (height x width).\n",
    "\n",
    "\n",
    "#### Grayscale Image\n",
    "\n",
    "If the image is black and white (a.k.a. grayscale), each pixel can be represented by a single number (commonly between 0 (black) and 255 (white)\n",
    "\n",
    "<img src=\"./images/numpy-grayscale-image.png\">\n",
    "\n",
    "\n",
    "#### Color Image\n",
    "\n",
    "If the image is colored, then each pixel is represented by three numbers - a value for each of Red, Green, and Blue. In that case we need a 3rd dimension (because each cell can only contain one number). So a colored image is represented by an ndarray of dimensions: (height x width x 3).\n",
    "\n",
    "<img src=\"./images/numpy-color-image.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember the color Leena image from before ? \n",
    "\n",
    "Let's try printing it and out to see how this color image is stored as a 3D (BGR) numpy matrix by OpenCV by running the snippet below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 512, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "img = cv2.imread('./images/lenna.png')\n",
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Image from NAO\n",
    "\n",
    "On the same note, once we get the image stream from NAO we will be able to process it and make sense of it.\n",
    "\n",
    "We get images from NAO's top camera and convert them to numpy array to process using OpenCV. This is done by Subscribing to `ALVideoDevice` proxy which gives raw image in form of pixel array which can be converted to numpy array using `np.asarray()` function and reshaped to a 3 dimensional color image of (height x width x 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subscribed to  nao_opencv_1\n",
      "Grabbed image:  320 x 240  numChannels= 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rama/.local/lib/python2.7/site-packages/ipykernel_launcher.py:44: DeprecationWarning: Assigning the 'data' attribute is an inherently unsafe operation and will be removed in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsubscribing  nao_opencv_1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from vision_definitions import kQVGA,kBGRColorSpace\n",
    "from naoqi import ALProxy\n",
    "\n",
    "NAO_IP=\"192.168.1.7\" # <YOUR_NAO_IP> or nao.local\n",
    "\n",
    "\n",
    "if __name__==\"__main__\":  # Should not run when imported\n",
    "\n",
    "    camera_index = 0 # Top camera\n",
    "   \n",
    "    # Proxy for ALVideoDevice\n",
    "    name = \"nao_opencv\"\n",
    "    video = ALProxy(\"ALVideoDevice\", NAO_IP, 9559)\n",
    "\n",
    "    # Subscribe to video device on a specific camera\n",
    "    # BGR for OpenCV\n",
    "    name = video.subscribeCamera(name,camera_index,kQVGA,kBGRColorSpace,30)\n",
    "    print \"Subscribed to \", name\n",
    "\n",
    "    try:\n",
    "        frame = None\n",
    "        # Keep Looping\n",
    "        while True:\n",
    "            # Get image\n",
    "            img = video.getImageRemote(name)\n",
    "\n",
    "            # Get image size attributes and pixel array buffer\n",
    "            imageWidth = img[0]\n",
    "            imageHeight = img[1]\n",
    "            numChannels = img[2]\n",
    "            imgBuffer = img[6]\n",
    "         \n",
    "            # Get OpenCV image (allocate on first pass)\n",
    "            if frame is None:\n",
    "                print 'Grabbed image: ',imageWidth,'x',imageHeight,' numChannels=',numChannels\n",
    "                frame=np.asarray(bytearray(imgBuffer), dtype=np.uint8)\n",
    "                frame=frame.reshape((imageHeight,imageWidth,3))\n",
    "            else:\n",
    "                frame.data=bytearray(imgBuffer)\n",
    "\n",
    "            # Display the frame to our screen\n",
    "            # NOTE : Do not run this code if your run your python in the robot\n",
    "            # as NAO has no screen to show\n",
    "            cv2.imshow(\"Frame\", frame)\n",
    "            \n",
    "            # Get the key pressed in the image window\n",
    "            key = cv2.waitKey(33)&0xFF\n",
    "            if  key == ord('q') or key == 27:\n",
    "                # Exit loop when 'q' or 'Esc' is pressed on the image window\n",
    "                break\n",
    "            \n",
    "    finally: # As fallback we'll make sure to unsubscribe\n",
    "        print \"Unsubscribing \",name\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "video.unsubscribe(name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/nao-camera.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Images for Dataset\n",
    "\n",
    "Now we know what NAO sees. Before we proceed to do gesture recognition on what NAO sees, we need to save some sample images on which we will train the AI model to recognize the gesture when it sees one.\n",
    "\n",
    "We will be recognizing a left hand and a right hand signal as the gestures. To do this, we will be saving atleast 10 images (more images lead to better recognition) to train for each left and right hand seperately and also a couple of images in each gesture to test the trained model.\n",
    "\n",
    "For convenience purposes, we will be saving the images for each gesture in their respective folders as follows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "+-----------+------------+--------------------+\n",
    "| Images For| Gesture    | Folder Path        |\n",
    "+-----------+------------+--------------------+\n",
    "| Training  | Left Hand  | /data/train/left/  |\n",
    "| Training  | Right Hand | /data/train/right/ |\n",
    "|  Testing  | Left Hand  | /data/test/left/   |\n",
    "|  Testing  | Right Hand | /data/test/right/  |\n",
    "+-----------+------------+--------------------+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting Absolute Path\n",
    "\n",
    "We will be saving the images in the `/ai4all/data/` folder in this project respectively as above. \n",
    "But every person might have this project in a different location in their device.\n",
    "Hence, we need to get the absolute path of this project as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/rama/workspace/ai4all\n"
     ]
    }
   ],
   "source": [
    "from os.path import dirname, abspath\n",
    "\n",
    "# Inside the script use abspath('') to obtain the absolute path of this script\n",
    "# Call os.path.dirname twice to get parent directory of this directory    \n",
    "\n",
    "parent_directory = dirname(dirname(abspath('')))\n",
    "\n",
    "print(parent_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running Script to Save Images\n",
    "\n",
    "Before we proceed to run the below script that saves images for training and test each for left and right hand gestures respectively. \n",
    "\n",
    "We need to replace `NAO_IP` with our NAO's IP.\n",
    "\n",
    "Also, based on what images you are trying to capture, replace `data_directory` correspondingly from above table.\n",
    "\n",
    "Now we can proceed to run the below code which will open a Popup of the image frame from NAO's camera and what it sees.\n",
    "\n",
    "Press `C` key to capture the image once you position your left/right hand in the center of the camera to save the image and you can keep doing so till the number of images you would like to save.\n",
    "\n",
    "Press `ESC` key when you are done to close the popup.\n",
    "\n",
    "Repeat the same for train (left and right) and test (left and right) respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subscribed to  nao_opencv_1\n",
      "Grabbed image:  320 x 240  nchannels= 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rama/.local/lib/python2.7/site-packages/ipykernel_launcher.py:53: DeprecationWarning: Assigning the 'data' attribute is an inherently unsafe operation and will be removed in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsubscribing  nao_opencv_1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "from os.path import dirname, abspath\n",
    "from vision_definitions import kQVGA,kBGRColorSpace\n",
    "from naoqi import ALProxy\n",
    "\n",
    "\n",
    "NAO_IP=\"192.168.1.7\" # <YOUR_NAO_IP> or nao.local\n",
    "\n",
    "\n",
    "if __name__==\"__main__\":  # Should not run when imported\n",
    "\n",
    "    camera_index = 0 # Top camera\n",
    "    image_count = 0 # We will use this to save image name as <image_count>.jpg\n",
    "    \n",
    "    \n",
    "    parent_directory = dirname(dirname(abspath(''))) # Will give ../ai4all/ which is the parent folder\n",
    "    \n",
    "    data_directory = \"/data/train/\" # Replace which images you are saving and corresponding folder path\n",
    "    \n",
    "    image_prefix = parent_directory + data_directory\n",
    "    image_suffix = \".jpg\"\n",
    "\n",
    "    # Proxy for ALVideoDevice\n",
    "    name = \"nao_opencv\"\n",
    "    video = ALProxy(\"ALVideoDevice\", NAO_IP, 9559)\n",
    "\n",
    "    # Subscribe to video device on a specific camera\n",
    "    # BGR for OpenCV\n",
    "    name = video.subscribeCamera(name,camera_index,kQVGA,kBGRColorSpace,30)\n",
    "    print \"Subscribed to \", name\n",
    "\n",
    "    try:\n",
    "        frame = None\n",
    "        # Keep Looping\n",
    "        while True:\n",
    "            # Get image\n",
    "            img = video.getImageRemote(name)\n",
    "\n",
    "            # Get image attributes\n",
    "            width = img[0]\n",
    "            height = img[1]\n",
    "            nchannels = img[2]\n",
    "            imgbuffer = img[6]\n",
    "            \n",
    "            # Get OpenCV image (allocate on first pass)\n",
    "            if frame is None:\n",
    "                print 'Grabbed image: ',width,'x',height,' nchannels=',nchannels\n",
    "                frame=np.asarray(bytearray(imgbuffer), dtype=np.uint8)\n",
    "                frame=frame.reshape((height,width,3))\n",
    "            else:\n",
    "                frame.data=bytearray(imgbuffer)\n",
    "\n",
    "            # Display the frame to our screen\n",
    "            # NOTE : Do not run this code if your run your python in the robot\n",
    "            # as NAO has no screen to show\n",
    "            cv2.imshow(\"Frame\", frame)\n",
    "\n",
    "            # Get the key pressed in the image window\n",
    "            key = cv2.waitKey(33)&0xFF\n",
    "            if  key == ord('q') or key == 27:\n",
    "                # Exit loop when 'q' or 'Esc' is pressed on the image window\n",
    "                break\n",
    "            elif key == 99:\n",
    "                # When 'c' key is pressed -> Capture/Save image\n",
    "                \n",
    "                # Let's crop the image frame so the focus is in center\n",
    "                upper_left = (80, 40)     #Crop: top left point\n",
    "                bottom_right = (230, 190) #Crop: bottom right point\n",
    "                cropped_frame = frame[upper_left[1] : bottom_right[1], upper_left[0] : bottom_right[0]]\n",
    "                \n",
    "                # Converting cropped color image to Grayscale\n",
    "                gray_frame = cv2.cvtColor(cropped_frame, cv2.COLOR_BGR2GRAY)\n",
    "                \n",
    "                # Lets resize thee grayscale image to 28 x 28 (height x width) for convenience\n",
    "                resized_frame = cv2.resize(gray_frame, dsize=(28, 28), interpolation=cv2.INTER_CUBIC)\n",
    "                \n",
    "                # Uncomment below line to see cropped and resized image frame\n",
    "                # cv2.imshow(\"Resized Frame\", resized_frame)\n",
    "                \n",
    "                # We will create a file path string as \n",
    "                # ../<parent directory>/.. <data directory>/ <image_count>.jpg\n",
    "                image_path = image_prefix + str(image_count) + image_suffix\n",
    "                \n",
    "                # imwrite function of OpenCV saves the resized image in the above specified folder\n",
    "                cv2.imwrite(image_path, resized_frame)\n",
    "                \n",
    "                # Change to image count so that the next image will be saved with next number\n",
    "                # If not every image we save will be overwritten by same file name\n",
    "                image_count = image_count+1\n",
    "\n",
    "    finally: # As fallback we'll make sure to unsubscribe\n",
    "        print \"Unsubscribing \",name\n",
    "       \n",
    "video.unsubscribe(name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saved Left Hand Signal Image\n",
    "\n",
    "<img src=\"../../data/train/left/16.jpg\">\n",
    "\n",
    "Saved Right Hand Signal Image\n",
    "\n",
    "<img src=\"../../data/train/right/2.jpg\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Now that we have got the image successfully, we proceed to recognize the gesture in the image."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
